{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control(No Success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from agent import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReacherBrain\n",
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "env_file_name = os.path.abspath(\"Reacher_Linux/Reacher.x86_64\")\n",
    "env = UnityEnvironment(file_name=env_file_name, no_graphics=True)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "print(brain_name)\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, w_scale=1.0):\n",
    "    nn.init.orthogonal_(layer.weight.data)\n",
    "    layer.weight.data.mul_(w_scale)\n",
    "    nn.init.constant_(layer.bias.data, 0)\n",
    "    return layer\n",
    "\n",
    "class ContinuousActorNet(nn.Module):\n",
    "    \"\"\"Deep Q Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=0):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(ContinuousActorNet, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "        self.mu   = nn.Sequential(layer_init(nn.Linear(state_size, 128), 1e-3), nn.ReLU(inplace=True),\n",
    "                                  layer_init(nn.Linear(128       , 128), 1e-3), nn.ReLU(inplace=True),\n",
    "                                  layer_init(nn.Linear(128       , 64 ), 1e-3), nn.ReLU(inplace=True),\n",
    "                                  layer_init(nn.Linear(64        , action_size), 1e-3), nn.Tanh())\n",
    "        self.std_val = 0\n",
    "    def forward(self, state, i_episode):\n",
    "        mean = self.mu(state)\n",
    "        sigma = max(0.5 - (0.1 - 0.5)/100 * i_episode, 0.01)\n",
    "        dist = torch.distributions.Normal(mean, torch.tensor(sigma))\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return action, log_prob, entropy\n",
    "    \n",
    "ContinuousActorNet(state_size, action_size)\n",
    "\n",
    "class ContinuousCriticNet(nn.Module):\n",
    "    \"\"\"Deep Deterministic Critic Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, seed=0):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(ContinuousCriticNet, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        # estimate advantage by TD error\n",
    "        self.model = nn.Sequential(layer_init(nn.Linear(state_size, 128), 0.5), nn.ReLU(inplace=True),\n",
    "                                   layer_init(nn.Linear(128       , 64), 0.5), nn.ReLU(inplace=True),\n",
    "                                   layer_init(nn.Linear(64, 1), 0.5), nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "\n",
    "ContinuousCriticNet(state_size)\n",
    "\n",
    "def train(params, n_episodes=1800, max_t=1000):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    \n",
    "    critic = ContinuousCriticNet(state_size).to(device)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=params.critic_lr)\n",
    "        \n",
    "    actor = ContinuousActorNet(state_size, action_size).to(device)\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=params.actor_lr)\n",
    "    \n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "\n",
    "        \n",
    "        for i in range(2):\n",
    "            env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "            state = env_info.vector_observations[0]           # get the current state\n",
    "            collect_value = []\n",
    "            collect_state = []\n",
    "            collect_reward = []\n",
    "            collect_done   = []\n",
    "            collect_log_prob = []\n",
    "            collect_entropy  = []\n",
    "            collect_next  = []\n",
    "            score = 0\n",
    "            for t in range(max_t):\n",
    "                state = torch.tensor(state, dtype=torch.float)\n",
    "                action, log_prob, entropy = actor(state, i_episode)\n",
    "                value = critic(state)\n",
    "                action = np.clip(action.cpu().data.numpy(), -1, 1)\n",
    "                env_info   = env.step(action)[brain_name]      # send the action to the environment\n",
    "                next_state = env_info.vector_observations[0]   # get the next state\n",
    "                reward     = env_info.rewards[0]               # get the reward\n",
    "                done       = env_info.local_done[0]            # see if episode has finished\n",
    "                collect_value.append(value)\n",
    "                collect_state.append(state)\n",
    "                collect_reward.append(reward)\n",
    "                collect_done.append(done)\n",
    "                collect_log_prob.append(log_prob)\n",
    "                collect_entropy.append(entropy)\n",
    "                collect_next.append(next_state)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break \n",
    "\n",
    "            v = critic(torch.tensor(next_state, dtype=torch.float)).detach()\n",
    "            returns = []\n",
    "            for reward, done in zip(reversed(collect_reward), reversed(collect_done)):\n",
    "                q = reward + params.gamma * (1 - done) * v\n",
    "                returns.insert(0, q)\n",
    "\n",
    "            values   = 1e2*torch.stack(collect_value)\n",
    "            states   = torch.stack(collect_state)\n",
    "            returns  = 1e2*torch.tensor(returns, dtype=torch.float).unsqueeze(dim=1)\n",
    "            log_prob = torch.stack(collect_log_prob)\n",
    "            entropy  = torch.stack(collect_entropy)\n",
    "            next_states = torch.tensor(collect_next, dtype=torch.float).unsqueeze(dim=1)\n",
    "        \n",
    "            loss_critic = F.mse_loss(returns, values)\n",
    "\n",
    "            critic_optimizer.zero_grad()\n",
    "            loss_critic.backward()\n",
    "            critic_optimizer.step()\n",
    "        print(f'loss_critic:{loss_critic}')\n",
    "        values = critic(states)\n",
    "        \n",
    "        adv = returns - values\n",
    "        tmp = -log_prob * adv.detach()\n",
    "        \n",
    "        loss_actor = torch.mean(-log_prob * adv.detach())\n",
    "        \n",
    "        actor_optimizer.zero_grad()    \n",
    "        loss_actor.backward()\n",
    "        actor_optimizer.step()\n",
    "\n",
    "\n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if i_episode % 100 == 0 and np.mean(scores_window)>=30.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            #torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores, filename, rolling_window=100):\n",
    "    \"\"\"Plot scores and optional rolling mean using specified window.\"\"\"\n",
    "    plt.figure(figsize=(19.20,10.80))\n",
    "    plt.plot(scores, color='b', linestyle='-', linewidth=0.75) \n",
    "    plt.title(\"Scores\");\n",
    "    rolling_mean = pd.Series(scores).rolling(rolling_window, min_periods=1).mean()\n",
    "    plt.plot(rolling_mean, color='r', linestyle='-', linewidth=0.75)\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_critic:1.5989094972610474\n",
      "loss_actor:0.6171565651893616\n",
      "Episode 1\tAverage Score: 0.57loss_critic:0.0\n",
      "loss_actor:0.0\n",
      "Episode 2\tAverage Score: 0.28loss_critic:0.0\n",
      "loss_actor:0.0\n",
      "Episode 3\tAverage Score: 0.19loss_critic:0.0\n",
      "loss_actor:0.0\n",
      "Episode 4\tAverage Score: 0.14loss_critic:0.14900000393390656\n",
      "loss_actor:0.03128720447421074\n",
      "Episode 5\tAverage Score: 0.19loss_critic:0.0\n",
      "loss_actor:0.0\n",
      "Episode 6\tAverage Score: 0.16loss_critic:0.09300000220537186\n",
      "loss_actor:0.016963299363851547\n",
      "Episode 7\tAverage Score: 0.17loss_critic:0.008999999612569809\n",
      "loss_actor:0.004105106461793184\n",
      "Episode 8\tAverage Score: 0.15loss_critic:0.0\n",
      "loss_actor:0.0\n",
      "Episode 9\tAverage Score: 0.14loss_critic:0.09799999743700027\n",
      "loss_actor:0.031199676916003227\n",
      "Episode 10\tAverage Score: 0.15loss_critic:0.3050000071525574\n",
      "loss_actor:0.06510315835475922\n",
      "Episode 11\tAverage Score: 0.21loss_critic:0.0\n",
      "loss_actor:0.0\n",
      "Episode 12\tAverage Score: 0.19loss_critic:0.0\n",
      "loss_actor:0.0\n",
      "Episode 13\tAverage Score: 0.18loss_critic:0.0\n",
      "loss_actor:0.0\n",
      "Episode 14\tAverage Score: 0.16loss_critic:0.061000000685453415\n",
      "loss_actor:0.018352041020989418\n",
      "Episode 15\tAverage Score: 0.17loss_critic:0.2879999876022339\n",
      "loss_actor:0.053801681846380234\n",
      "Episode 16\tAverage Score: 0.20loss_critic:0.0\n",
      "loss_actor:0.0\n",
      "Episode 17\tAverage Score: 0.19loss_critic:0.0\n",
      "loss_actor:0.0\n",
      "Episode 18\tAverage Score: 0.18loss_critic:0.0\n",
      "loss_actor:0.0\n",
      "Episode 19\tAverage Score: 0.17loss_critic:0.032999999821186066\n",
      "loss_actor:0.008238213136792183\n",
      "Episode 20\tAverage Score: 0.16loss_critic:0.1889999955892563\n",
      "loss_actor:0.04875839874148369\n",
      "Episode 21\tAverage Score: 0.18loss_critic:0.12200000137090683\n",
      "loss_actor:0.027574878185987473\n",
      "Episode 22\tAverage Score: 0.19loss_critic:0.0\n",
      "loss_actor:0.0\n",
      "Episode 23\tAverage Score: 0.18loss_critic:0.0\n",
      "loss_actor:0.0\n",
      "Episode 24\tAverage Score: 0.17loss_critic:0.0\n",
      "loss_actor:0.0\n",
      "Episode 25\tAverage Score: 0.16loss_critic:0.026000000536441803\n",
      "loss_actor:0.008908605203032494\n",
      "Episode 26\tAverage Score: 0.16loss_critic:0.04800000041723251\n",
      "loss_actor:0.006760967429727316\n",
      "Episode 27\tAverage Score: 0.16loss_critic:0.1420000046491623\n",
      "loss_actor:0.04041559621691704\n",
      "Episode 28\tAverage Score: 0.17"
     ]
    }
   ],
   "source": [
    "class Params:\n",
    "    \"\"\"Set up configuration here.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.__dict__.update(**{\n",
    "            'gamma' : 0.5,            # discount factor\n",
    "            'actor_lr' : 1e-6,               # learning rate \n",
    "            'critic_lr' : 1e-3,\n",
    "            'entropy_weight': 1e-3\n",
    "})\n",
    "        \n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "params = Params()\n",
    "\n",
    "\n",
    "# agent = ContinuousACagent(state_size=state_size, action_size=action_size, params=params, device=device)\n",
    "scores = train(params, n_episodes=50, max_t=1000)\n",
    "\n",
    "#filename = model_name + '.png'\n",
    "plot_scores(scores=scores, filename=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
